# GCP IMPs
## If the CLI is using wrong account
gcloud auth login
gcloud config list  # Confirm account = shubham29.gattani@gmail.com

# CREATE BUCKET
gsutil mb gs://YOUR_BUCKET_NAME 
gsutil mb gs://mlops-ope1-shubhamg-practice-bucket # Create bucket
gsutil ls gs://<your-bucket>/feast/ # To check the registry file.

# GIT THINGS
git config --global user.name "Shubham-Gattani"
git config --global user.email "shubham29.iitd@gmail.com"

# Creating VENV
python -m venv venv
source venv/bin/activate
pip install gcsfs pandas==2.3.3 numpy==2.3.3 scikit-learn==1.7.2 joblib feast==0.54.0 'feast[gcp]'
pip install dvc==3.63.0 dvc-gs==3.0.2 pytest mlflow==3.5.0 ipykernel

python -m ipykernel install --user --name=venv_name --display-name "Python (venv_name)" # Register the venv with Jupyter

# DVC THINGS

dvc init

## Getting v0 of the data
dvc get https://github.com/Shubham-Gattani/Continuous-Learning-Repo temp/v0
dvc get <repo_url> <path_inside_repo> [--rev <branch_or_tag>]
dvc add v0

# Create features for v0 of data
python python_files/feature_engineering.py --input_dir v0 --output_dir processed_data/processed_v0


dvc remote add -d gcsremote gs://mlops-ope1-shubhamg-practice-bucket
dvc remote list # Check if the bucket is actually configured
dvc push
NOTE: DVC remote is NOT used for Feast ingestion.
Feast needs BigQuery → BigQuery needs GCS parquet uploaded manually using gsutil, NOT dvc push.


# FEAST related THINGS

## Initialize Feast repo

## Step 1️⃣ — Initialize Feast repo
feast init feature_repo # Inside your project root
rm feature_repo/example_repo.py # safely delete the sample file:

## Step 2️⃣ — Configure Feast to use BigQuery; edit feature_store.yaml

## Step 3️⃣ — Define Feature Views; create a file feature_repo/features.py
1. Define a stock entity (e.g., stock symbol or stock name).
2. Register features: rolling_avg_10, volume_sum_10, and target.
3. Use the processed data from processed_data/ (or data/processed_v0/, depending on where you saved it).

NOTE: Feast requires an entity key to uniquely identify each row. So, while processing the data, 
you should make sure that the processed data has a seperated column which can be used as an entity. In this case, it is the "stock_symbol"

NOTE: Feast and BigQuery both REQUIRE timezone-naive UTC timestamps. Hence, 
after you process the data, first of all, merge it into a file and save it as a parquet file, 
convert the timestamp column into a atleast timestamp[us], it should never be timestamp[ns]. Then,
Upload the merged file to GCS: `gsutil cp <merged_file_location> gs://<bucket_name>/feast_parquet/..`
Then, Load it into BigQuery
        bq load \
            --source_format=PARQUET \
            --autodetect \
            mlops-iris-week1-graded:feast.stock_features_all \
            gs://<bucket>/feast_parquet/stock_features_all.parquet

NOTE: Always remove all the NULL values from the features which are used to create feature_views

## Step 4️⃣ — feast apply
cd feature_repo
feast apply  # run from inside feature_repo folder
    1️⃣ Parses your feature_store.yaml
    Feast reads:
        Where to store the registry (you configured it in GCS)
        What’s your offline store (BigQuery)
        What’s your online store (SQLite for now)
        Your project name (stock_prediction)

    2️⃣ Scans your feature repo
    It looks inside your feature_repo/ folder for Python files (like features.py)
    and finds:
        Entities (like stock_symbol)
        Feature views (like stock_features_view)
        Data sources (your processed CSVs or BigQuery tables)

    3️⃣ Builds the registry
    Feast then writes metadata about all of these into a registry file:
        `gs://<your-bucket>/feast/registry.db`
    That registry is like the “catalog” for all feature definitions in your project.
    It tells Feast (and your CI jobs later) which features exist, their schema, source, and lineage.

    4️⃣ Creates or validates BigQuery dataset
    If your offline store (feast_stock_features) doesn’t exist in BigQuery yet, Feast creates it automatically under your GCP project.
    This is where Feast will later store the ingested feature data.

    ✅ After feast apply:
        Feast knows all your feature definitions.
        BigQuery dataset is ready.
        GCS registry file is updated.

    NOTE:
    1. After applying `feast apply` command, always go to GCP BQ and confirm is a dataset is formed or not. Or simply run `bq ls`
    2. If you cannot see anything in BQ, then you may need to assign proper roles to the current user
        using `gcloud auth list` and then run
        gcloud projects add-iam-policy-binding mlops-iris-week1-graded \
        --member="user:shubham29.gattani@gmail.com" \
        --role="roles/bigquery.admin"




## Step 5️⃣: MATERIALIZE
feast materialize-incremental $(date +%Y-%m-%d)

✅ What this will do

Materialization = moving data FROM the offline store (BigQuery) INTO the online store (SQLite).
Feast does NOT ingest files during materialization — ingestion already 
happened when you loaded parquet → BigQuery.

Feast will:
    Read all your .pq files from:
    ../processed_data/processed_v0/

    Convert them into BigQuery tables.
    Write them into your BQ dataset (feast or feast_stock_features, depending on your YAML).

    Create tables like: feast.stock_features
    Store each row keyed by: (stock_symbol, timestamp)
    What you should see in BigQuery after running it
        Inside your dataset (feast or feast_stock_features), you should see a table named:  stock_features

    NOTE:
        1. The offline store inside your yaml should be "BigQuery" and youe features.py should also mention "BigQuerySource".
            Only then the materialization will work.
        2. When you make any change in the parquet files, then,
            re-upload them to GCS bucket `gsutil cp processed_data/stock_features_all.parquet gs://<your-bucket>/feast_parquet/`
            delete the old registry `gsutil rm gs://mlops-ope1-shubhamg-practice-bucket/feast/registry.db`
            delete the old table from BQ: `bq rm -f mlops-iris-week1-graded:<dataset_name>.<table_name>`
                                            bq rm -f mlops-iris-week1-graded:feast.stock_features_all
            load the new data again from GCS bucket to BQ:
                                                        bq load \
                                                            --source_format=PARQUET \
                                                            --autodetect \
                                                            mlops-iris-week1-graded:feast.stock_features_all \
                                                            gs://<your-bucket>/feast_parquet/stock_features_all.parquet
            
✅ 8. (Optional but helpful): Add “Workflow for updating data”

    This exact sequence is important:

        Regenerate merged parquet
        Upload to GCS
        Delete old BQ table
        Load new parquet into BQ
        Delete Feast registry
        feast apply
        feast materialize-incremental
    
    This is your “data refresh workflow.”





SQL queries:
    1. gives exact null counts for each column.

    SELECT
    COUNTIF(timestamp IS NULL) AS null_timestamp,
    COUNTIF(rolling_avg_10 IS NULL) AS null_rolling_avg_10,
    COUNTIF(volume_sum_10 IS NULL) AS null_volume_sum_10,
    COUNTIF(target IS NULL) AS null_target,
    COUNTIF(stock_symbol IS NULL) AS null_stock_symbol
    FROM `mlops-iris-week1-graded.feast.stock_features_all`;

    SELECT
    COUNTIF(col1 IS NULL) AS col1_nulls,
    COUNTIF(col2 IS NULL) AS col2_nulls,
    COUNTIF(col3 IS NULL) AS col3_nulls
    FROM `project.dataset.table`;


2. Confirm timestamp type of a single column
    SELECT data_type
    FROM `mlops-iris-week1-graded.feast.INFORMATION_SCHEMA.COLUMNS`
    WHERE table_name = 'stock_features_all'
    AND column_name = 'timestamp';


##############  Next major milestone: Model training with Feast as the feature provider

Create test_feast_training.py skeleton  # NOTE: For this dataset, we don't need any PIT joins.

# ✅ Summary of steps after completing Feast setup (v0)

### ✅ Files created

**1. `python_files/training_v0.py`**

* Loads merged data from BigQuery
* Trains 2 LogisticRegression models (hyperparam tuning)
* Logs runs to MLflow
* Selects best model
* Saves: `models/best_logistic_model_v0.joblib`

**2. `mlruns/` folder**

* Automatically created by MLflow
* Stores experiment runs, params, metrics, artifacts

**3. `models/best_logistic_model_v0.joblib.dvc`**

* DVC metadata for the best model
* Model file stored in remote DVC storage (GCS)

---

# ✅ Commands executed

```bash
# Run training (creates mlruns + model)
mlflow ui # To see experiments via MLFlow GUI
python python_files/training_v0.py

# Track the best model with DVC
dvc add models/best_logistic_model_v0.joblib

git add models/best_logistic_model_v0.joblib.dvc .gitignore \
        python_files/training_v0.py python_files/test_feast_training.py \
        common-commands.txt
git commit -m "Add best logistic model for v0 (DVC tracked)"

dvc push
git push

# Tag the v0 pipeline for easy rollback
git tag -a v0 -m "Completed v0 pipeline"
git push origin v0
```

---

# ✅ Final output of v0 iteration

* ✅ Clean training script (`training_v0.py`)
* ✅ Two MLflow runs logged
* ✅ Best model saved & tracked in DVC
* ✅ Code + metadata committed to Git
* ✅ v0 pipeline tagged as `v0`

---

Great — **Iteration 2 (v1)** is where you demonstrate *continuous learning*, which earns high marks.

We will follow a clean and simple 5-step plan.

################ ITERATION 2 ON V1 ###################


# ✅ **ITERATION 2 (v1) — PLAN**

### ✅ Step 1 — Download v1 data
dvc get https://github.com/Shubham-Gattani/Continuous-Learning-Repo temp/v1 # From project root

### ✅ Step 2 — Process v1 data using `feature_engineering.py`
python python_files/feature_engineering.py --input_dir v1 --output_dir processed_data/processed_v1


### ✅ Step 3 — Merge v0 + v1 into a new training dataset
python python_files/merge_v0_v1.py


### ✅ Step 4 — Upload merged dataset → GCS → Load to BigQuery

# Upload merged_v1 parquet to GCS
gsutil cp processed_data/merged_files/stock_features_all_v1.parquet \
gs://mlops-ope1-shubhamg-practice-bucket/feast_parquet/ 

bq rm -f mlops-iris-week1-graded:feast.stock_features_all # Delete old table

# Load new v1 merged parquet into BigQuery

### ✅ Step 5 — Train v1 model → MLflow → pick best → DVC track
Create File: python_files/training_v1.py
    ✅ new MLflow experiment: stock_prediction_v1
    ✅ new model output: best_logistic_model_v1.joblib
    ✅ still uses the same BigQuery table, which now contains v0+v1 data


### ✅ Step 6 — Tag as `v1`
dvc add v1/ models/best_logistic_model_v1.joblib
git add v1.dvc models/best_logistic_model_v1.joblib.dvc python_files/training_v1.py
git commit -m "Add best logistic model for v1 (DVC tracked)"
dvc push
git push
git tag -a v1 -m "Completed v1 pipeline (v0+v1 training, MLflow, DVC)"
git push origin v1






